{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOTDtUftZ5KD44HoY8/oBdc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinjmann/transformer_summarizer/blob/main/TransformerSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a transformer based summarizer.\n",
        "\n",
        "The dataset comes from here https://github.com/Alex-Fabbri/Multi-News which is part of the tensor flow data set available in the following package"
      ],
      "metadata": {
        "id": "n2WbEFpzXDmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tfds-nightly"
      ],
      "metadata": {
        "id": "XfBQkL9qXCJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "JH5OV9VmYarH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = tfds.builder('multi_news')"
      ],
      "metadata": {
        "id": "T2vpBBDDYfGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder.download_and_prepare()"
      ],
      "metadata": {
        "id": "rP8Wwu5_Y3wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al /root/tensorflow_datasets/multi_news/1.0.0/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMNekbbRZiT3",
        "outputId": "fc0610b9-b0f0-428b-d59f-9343b9bf7456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 686796\n",
            "drwxr-xr-x 2 root root     4096 Jan 31 02:33 .\n",
            "drwxr-xr-x 3 root root     4096 Jan 31 02:33 ..\n",
            "-rw-r--r-- 1 root root     3113 Jan 31 02:33 dataset_info.json\n",
            "-rw-r--r-- 1 root root      464 Jan 31 02:33 features.json\n",
            "-rw-r--r-- 1 root root 70697817 Jan 31 02:33 multi_news-test.tfrecord-00000-of-00001\n",
            "-rw-r--r-- 1 root root 70464259 Jan 31 02:33 multi_news-train.tfrecord-00000-of-00008\n",
            "-rw-r--r-- 1 root root 69497487 Jan 31 02:33 multi_news-train.tfrecord-00001-of-00008\n",
            "-rw-r--r-- 1 root root 69628388 Jan 31 02:33 multi_news-train.tfrecord-00002-of-00008\n",
            "-rw-r--r-- 1 root root 71909427 Jan 31 02:33 multi_news-train.tfrecord-00003-of-00008\n",
            "-rw-r--r-- 1 root root 70475688 Jan 31 02:33 multi_news-train.tfrecord-00004-of-00008\n",
            "-rw-r--r-- 1 root root 69650383 Jan 31 02:33 multi_news-train.tfrecord-00005-of-00008\n",
            "-rw-r--r-- 1 root root 68935097 Jan 31 02:33 multi_news-train.tfrecord-00006-of-00008\n",
            "-rw-r--r-- 1 root root 73055971 Jan 31 02:33 multi_news-train.tfrecord-00007-of-00008\n",
            "-rw-r--r-- 1 root root 68925483 Jan 31 02:33 multi_news-validation.tfrecord-00000-of-00001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all the data in the dataset"
      ],
      "metadata": {
        "id": "Lhe2WezI-nz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tfds.as_numpy(tfds.load('multi_news', split='train', as_supervised=True, batch_size=-1))"
      ],
      "metadata": {
        "id": "xGdytSXEaT5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tfds.as_numpy(tfds.load('multi_news', split='validation', as_supervised=True, batch_size=-1))"
      ],
      "metadata": {
        "id": "WgjMT5Q55KIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents, summaries = train_ds\n",
        "val_docs, val_summaries = val_ds"
      ],
      "metadata": {
        "id": "cleq3_guacuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample document entry\n",
        "documents[0].decode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "7gVYsiwcglNx",
        "outputId": "828a1ed6-c6f7-4105-b143-950a71135a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Flag Flap Underscores Trump\\'s Strained Relationship With McCain \\n  \\n Enlarge this image toggle caption Mandel Ngan/AFP/Getty Images Mandel Ngan/AFP/Getty Images \\n  \\n Updated at 9:37 p.m. ET \\n  \\n The beginning of the national memorial for Sen. John McCain, R-Ariz., has been marred by a fight over a sign of public respect, as President Trump initially avoided issuing a proclamation to lower flags to half-staff at all federal properties in McCain\\'s honor. \\n  \\n Flags were lowered at government buildings across Washington and across the country Saturday evening after McCain died, as is standard practice for a sitting member of Congress. \\n  \\n But on Monday morning the flag atop the White House was back at full-staff, causing some to ask whether Trump\\'s strained relationship with McCain had played into the decision to not keep it lowered. The lack of a proclamation was viewed by some as a disrespectful act reflecting the president\\'s dislike for McCain, which Trump continued to express publicly, even as recently as last week. \\n  \\n Hours after reporters questioned the White House about the move and the president ignored multiple press attempts to ask his reaction to McCain\\'s death, the White House flag was eventually lowered to half-staff Monday afternoon. \\n  \\n Trump said in a statement released shortly afterward: \"Despite our differences on policy and politics, I respect Senator John McCain\\'s service to our country and, in his honor, have signed a proclamation to fly the flag of the United States at half-staff until the day of his interment.\" \\n  \\n Until that point, Trump had not issued a formal statement on McCain or commented on his service to the country, instead tweeting brief condolences Saturday to the senator\\'s family. \\n  \\n Later Monday evening at a dinner with evangelical leaders, Trump made his first public comments since the senator\\'s death Saturday. \"We very much appreciate everything that Senator McCain has done for our country,\" the president said. \\n  \\n \"Our hearts and prayers are going to the family,\" Trump also said, echoing his tweet. \"There\\'s going to be a lot of activity over the next number of days.\" \\n  \\n The dust-up over the flag was viewed as particularly insulting by veterans. McCain was a retired captain in the Navy and the son and grandson of two four-star Navy admirals. He was held for 5 1/2 years as a prisoner of war in Vietnam after the attack aircraft he was piloting was shot down during a bombing raid over Hanoi. He was tortured but refused early release because it would have meant leaving ahead of other soldiers who had been captured before him. \\n  \\n American Legion National Commander Denise Rohan issued a letter before Trump\\'s late afternoon written statement, appealing to the president to follow the custom he had used in recent deaths of national figures. \\n  \\n \"The American Legion urges the White House to follow long-established protocol following the death of prominent government officials,\" she wrote. \"Mr. President, just this year, you released presidential proclamations noting the deaths of Barbara Bush and Billy Graham. Senator John McCain was an American hero and cherished member of The American Legion.\" \\n  \\n The veterans group AMVETS also issued a statement calling the president\\'s actions since McCain\\'s death deeply disappointing. \\n  \\n \"It\\'s outrageous that the White House would mark American hero John McCain\\'s death with a two-sentence tweet, making no mention of his heroic and inspiring life,\" said AMVETS National Executive Director Joe Chenelly. \"And by lowering flags for not one second more than the bare minimum required by law, despite a long-standing tradition of lowering flags until the funeral, the White House is openly showcasing its blatant disrespect for Senator McCain\\'s many decades of service and sacrifice to our country as well as the service of all his fellow veterans.\" \\n  \\n And in what could be viewed as a subtle slap at Trump, the Canadian Embassy in Washington also posted a picture showing that it had lowered its flag to honor McCain. \\n  \\n \" Senator John McCain was a long-serving U.S. Senator, naval officer, strong advocate for NATO, and a good friend to Canada. The flag at the Embassy has been lowered to half-mast in his honour,\" the embassy tweeted. \\n  \\n This week McCain will lie in state in both the U.S. and Arizona capitols and will be memorialized at a funeral at his family\\'s church in Phoenix and at a service at the National Cathedral in Washington, D.C. He will be buried at the cemetery at the U.S. Naval Academy in Annapolis, Md., on Sunday. \\n  \\n According to the U.S. Code, the rules for a member of Congress state that flags are to be lowered on the day of the death plus one additional day. In other instances the White House has issued a proclamation extending the period to keep the flags at half-staff. \\n  \\n It was no secret that the president and McCain frequently clashed. The Washington Post reported that when White House press secretary Sarah Sanders and other officials initially prepared a statement in Trump\\'s name praising McCain, the president rejected that plan, opting instead for his Saturday tweet. \\n  \\n Senate Majority Leader Mitch McConnell, R-Ky., and Senate Minority Leader Chuck Schumer, D-N.Y., sent a joint letter to the Department of Defense on Sunday requesting that flags \"at all government buildings and installations\" remain at half-staff through McCain\\'s burial on Sunday. \\n  \\n \"We\\'ve received the letter and we\\'ll be working with Senator Schumer and Senator McConnell,\" said Tom Crosson, a Pentagon spokesman. But with the White House proclamation issued late on Monday, the Pentagon can point to that as the directive responsive to the congressional request. \\n  \\n NPR\\'s Tom Bowman contributed to this report. ||||| Tension between President Donald Trump and Sen. John McCain made an already sorrowful moment even more strained for Republican senators, like Bob Corker (pictured), returning to the Capitol on Monday. | Toya Sarno Jordan/Getty Images GOP senators ding White House over McCain flag dust-up \\'I think all of us are focused on the good of John McCain, and not the pettiness of others. I’ll just leave it at that,\\' says Sen. Bob Corker. \\n  \\n President Donald Trump\\'s delay in honoring the late Sen. John McCain left some Republican senators questioning on Monday why the president failed to lower the White House flag to half-staff sooner. \\n  \\n Trump’s proclamation Monday to order the White House flag to half-staff in McCain’s honor — after a raising of the flag on Monday morning drew criticism from veterans groups — left even some of the president’s congressional GOP allies scratching their heads. As Americans on both sides of the aisle came together to mourn McCain, the move seemed to strike the late Arizona senator’s bereaved Republican colleagues as a needless unforced error. \\n  \\n Story Continued Below \\n  \\n Sen. Orrin Hatch (R-Utah) said the Monday flap over White House flags rising while Capitol flags remained at half-staff for McCain “should not have happened.” \\n  \\n “That should have been automatic,” Hatch told reporters. “You do things that are sensible. And sensitive.” \\n  \\n Although official code governing the display of the flag calls for a lowering to half-staff for one full day after a member of Congress dies, which the White House had done following McCain’s death on Saturday, usual protocol for particularly high-ranking public officials has involved keeping the flag lowered until the day of interment. \\n  \\n Trump signed a proclamation making that happen on Monday afternoon, only after veterans groups had begun speaking out — and began his statement with an acknowledgment of his and McCain’s “differences on policy and politics.” After tangling with Trump during the 2016 campaign, McCain was unafraid to openly challenge Trump’s agenda and view of the world, including an implicit jab at the president in a farewell statement released publicly on Monday. \\n  \\n Sign up here for POLITICO Huddle A daily play-by-play of congressional news in your inbox. Email Sign Up By signing up you agree to receive email newsletters or alerts from POLITICO. You can unsubscribe at any time. \\n  \\n Trump declined to answer repeated questions from White House reporters about McCain earlier on Monday, but later appeared to warm to honoring the late senator yet again during a dinner with evangelical leaders. “We very much appreciate everything that Sen. McCain has done for our country,\" Trump said, according to a pool report. \\n  \\n That tension between Trump and McCain made an already sorrowful moment even tenser for Republican senators returning to the Capitol on Monday. \\n  \\n “I could not understand why the administration had the flag lowered for such a brief period of time,” Sen. Susan Collins (R-Maine) said. \\n  \\n Asked whether Trump was letting his strained relationship with the late six-term Arizonan get in the way of honoring McCain, Collins added: “It certainly looks that way, but I can’t speak for his motive. I’m just glad that the decision’s been reversed.” \\n  \\n Sen. Cory Gardner (R-Colo.), chief of the GOP Conference’s campaign arm, declined to address the White House’s decision-making on the flag and tried to pull the conversation back to McCain. \\n  \\n “I’m not going to get into that,” Gardner said. “What I am going to say is, this week is about John McCain, his legacy, his lifetime of service to this country.” \\n  \\n Sen. Shelley Moore Capito (R-W.Va.) readily volunteered that she was glad to see the White House return the flags to half-staff. \\n  \\n “I mean, come on, he’s a national hero, much-beloved, and I’m pleased that we’re doing the right thing,” Capito said, circling back to clarify that “I shouldn’t say ‘we’ — the right thing is being done.” \\n  \\n Trump’s proclamation on raising the White House flag came soon after Senate Majority Leader Mitch McConnell (R-Ky.) and Minority Leader Chuck Schumer (D-N.Y.), who each delivered heartfelt speeches in McCain’s honor Monday, asked the Pentagon for assistance in flying flags at half-staff on all government buildings. The Senate also unanimously passed a resolution in McCain’s honor late Monday. \\n  \\n Both McConnell and Schumer are scheduled to deliver remarks Friday at a ceremony in honor of McCain, who will lie in state in the Capitol Rotunda. Vice President Mike Pence is slated to speak on behalf of the administration, while Trump will stay away from both that event and McCain’s Saturday memorial service in Washington. \\n  \\n Schumer has proposed renaming the Russell Senate Office Building, where McCain’s office was located, in honor of the late GOP presidential nominee and decorated veteran. McConnell has yet to fully endorse that idea, saying Monday that he would consult with fellow senators on it. But Sen. Jeff Flake (R-Ariz.) signed on as a cosponsor of Schumer’s plan, and other Republicans signaled potential support. \\n  \\n “I think I’d be in favor of naming almost any building for McCain,” Sen. Chuck Grassley (R-Iowa) told reporters, “but I’m not sure that I want to make a decision on a specific building at this point.” \\n  \\n Although the late former Sen. Richard Russell (D-Ga.) “is somebody that was obviously a huge figure,” Sen. Bob Corker (R-Tenn.) said, “it is an era that’s gone by.” Speculating about potential opposition to the renaming, Corker quipped that “I don’t know who would want to vote against naming a building after somebody who just passed.” \\n  \\n The Foreign Relations chairman also offered a subtle jab at Trump when asked about the White House’s stumble on honoring McCain with the flag at half-staff. \\n  \\n “I think all of us are focused on the good of John McCain, and not the pettiness of others,” Corker said. “I’ll just leave it at that.” \\n  \\n John Bresnahan contributed to this report. \\n  \\n CORRECTION: An initial version of this story incorrectly identified the party of former Sen. Richard Russell. ||||| After ignoring repeated questions all day about whether he would say anything about his political nemesis, John McCain, President Donald Trump finally spoke about him Monday evening, saying \"our hearts and prayers are going to the family of Senator John McCain ... and we very much appreciate everything Sen. McCain has done for our country.\" \\n  \\n Interested in John McCain? Add John McCain as an interest to stay up to date on the latest John McCain news, video, and analysis from ABC News. Add Interest \\n  \\n The president was speaking to a dinner with evangelical leaders. He extended prayers and condolences to the victims of the Jacksonville, Florida shooting as well. \\n  \\n The president\\'s comments came a few hours after the White House abruptly returned its flag to half-staff Monday afternoon in honor of the late senator. Trump had ignored questions about McCain in the Oval Office Monday morning and at other events as well. \\n  \\n The president issued a statement Monday afternoon explaining why the flag was again lowered -- after facing widespread criticism for not keeping it at half-staff. \\n  \\n Leah Millis/Reuters \\n  \\n \"Despite our differences on policy and politics, I respect Senator John McCain’s service to our country and, in his honor, have signed a proclamation to fly the flag of the United States at half-staff until the day of his interment,\" the president said. \\n  \\n \"I have asked Vice President Mike Pence to offer an address at the ceremony honoring Senator McCain at the United States Capitol this Friday,\" the statement continued. \\n  \\n \"At the request of the McCain family, I have also authorized military transportation of Senator McCain’s remains from Arizona to Washington, D.C., military pallbearers and band support, and a horse and caisson transport during the service at the United States Naval Academy. Finally, I have asked General John Kelly, Secretary James Mattis, and Ambassador John Bolton to represent my Administration at his services,\" the president said. \\n  \\n But throughout much of the day, Trump had ignored almost a dozen questions from ABC News chief White House correspondent Jonathan Karl and reporters on McCain. \\n  \\n In contrast, the president\\'s daughter Ivanka Trump praised McCain as a \"hero\" at a women\\'s empowerment event in Washington, D.C. \\n  \\n “As we gather here today I want to extend my deepest sympathies to the family of Sen John McCain –an American patriot who served our country with distinction for more than six decades,\" said Trump. \"The nation is united in its grief and the world mourns the loss of a true hero and a great statesman.\" \\n  \\n \"Why won\\'t you say anything about John McCain?\"@jonkarl asked Pres. Trump ten times today to comment on Senator John McCain, who passed away this weekend at age 81. The president did not respond. https://t.co/HebITj8csN pic.twitter.com/iwKYhGRaNj — ABC News (@ABC) August 27, 2018 \\n  \\n White House officials had returned the U.S. flag to full-staff around midnight, ABC News senior White House correspondent Cecilia Vega told “Good Morning America” host George Stephanopoulos Monday. \\n  \\n The White House flag could then be seen for some time flying at full-staff while the banners surrounding the Washington Monument were at half-staff. \\n  \\n karentravers/Twitter \\n  \\n The initial White House flag-lowering that lasted less than 48 hours broke with precedent that it remain lowered until burial. \\n  \\n The federal code states that the flag shall be lowered on the day of death and the following day for a sitting member of Congress. Flying it at half-staff for an extended period of time is at the discretion of the president. \\n  \\n Susan Walsh/AP, FILE \\n  \\n Meanwhile, Trump inititialy rejected his aides’ recommendation to issue a statement praising the late Arizona senator, The Washington Post reported Sunday. \\n  \\n But Trump did release a tweet of condolences. \\n  \\n My deepest sympathies and respect go out to the family of Senator John McCain. Our hearts and prayers are with you! — Donald J. Trump (@realDonaldTrump) August 26, 2018 \\n  \\n The McCain family had reportedly asked the president not to attend McCain’s funeral even before he died this weekend at age 81. \\n  \\n Two former presidents -- Barack Obama and George W. Bush -- will deliver eulogies Saturday. Vice President Mike Pence has also been invited to the funeral. ||||| WASHINGTON (Reuters) - The White House lowered its U.S. flag to half-staff, raised it back up and on Monday lowered it again after the death of Senator John McCain, in an unusual and confusing break with protocol on the passing of a national leader. \\n  \\n McCain, a prisoner of war in Vietnam, longtime U.S. senator from Arizona and 2008 Republican presidential nominee, died of brain cancer on Saturday at age 81. That prompted many Americans to lower flags to half-staff, a traditional gesture of honor. \\n  \\n But President Donald Trump, who had clashed with fellow Republican McCain over various issues and said during his campaign that the senator was “not a war hero,” wavered in his approach to what presidents normally treat as a gesture of courtesy and respect. \\n  \\n Trump’s White House lowered its flag on Saturday, then raised it back following the minimum period under law. Trump also delayed issuing the customary proclamation for flags to remain at half-staff for longer than the two-day minimum. \\n  \\n Finally, under pressure from veterans and members of Congress, Trump said in a statement later on Monday that he respected McCain’s service to the nation and had ordered flags to half-staff. \\n  \\n In a letter to Trump on its Facebook page, the American Legion veterans group had urged the White House “to follow long-established protocol following the death of prominent government officials.” The Legion described McCain as a “cherished member.” \\n  \\n After a day of ignoring shouted questions about McCain, Trump broke his silence during a gathering of evangelical leaders at the White House on Monday evening. \\n  \\n A combination of three photographs shows the U.S. flag atop the White House flying at half staff Sunday morning August 26 in honor of the death of Senator John McCain (L), back at full staff less than 24 hours later on Monday morning August 27 (C) and then back down to half-staff Monday afternoon (R) in Washington, U.S., August 27, 2018. REUTERS/Joshua Roberts, Kevin Lamarque and Leah Millis \\n  \\n “Our hearts and prayers are going to the family of Senator John McCain ... and we very much appreciate everything Senator McCain has done for our country,” he told the religious leaders. \\n  \\n Through most of Monday, confusion reigned across the federal government, with flags flying at half-staff over the U.S. Capitol and at hundreds of national parks, but at full-staff over the Pentagon and the U.S. Supreme Court. \\n  \\n The U.S. Department of Homeland Security had issued a government-wide notification after McCain’s death to lower flags at U.S. facilities but rescinded it on Monday, leaving the decision to staff at individual sites, according to an official. \\n  \\n Slideshow (2 Images) \\n  \\n ‘SOMEWHAT SHOCKING’ \\n  \\n Presidents normally follow Congress’ lead on the death of a prominent lawmaker and order flags lowered until sunset on the day of burial. Critics of the president saw his reticence as a final slight against McCain. \\n  \\n “I doubt you could find a comparable situation where the president doesn’t order the flag flown at half-mast until the funeral,” said John Lawrence, history professor at the University of California’s Washington Center. \\n  \\n “The disparity between the Congress and White House policy is obviously noticeable and somewhat shocking.” \\n  \\n McCain was a frequent Trump critic and his family has said he did not want the president to attend his funeral. \\n  \\n A family spokesman issued a farewell statement from McCain in which he said of the United States: “We weaken our greatness when we confuse our patriotism with tribal rivalries ... We weaken it when we hide behind walls, rather than tear them down, when we doubt the power of our ideals, rather than trust them to be the great force for change they have always been.” |||||'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()"
      ],
      "metadata": {
        "id": "N03P1pu0faUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nchar = 1  # nchar corresponds to char n-grams that I'll use as labels"
      ],
      "metadata": {
        "id": "aWAxkRE-gVQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "lu4hi95zg8ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect the bigrams that appear in the dataset. Note their frequency"
      ],
      "metadata": {
        "id": "CMJCuAAt_InQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict = {}"
      ],
      "metadata": {
        "id": "NAcofOvoiicf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('documents')\n",
        "for document in tqdm(documents):\n",
        "  document = document.decode()[:-5]\n",
        "  # for bi in set([document[i:i+nchar] for i in range(0, len(document), nchar)]):\n",
        "  for bi in set(document):\n",
        "    if bi not in freq_dict:\n",
        "      freq_dict[bi] = 0\n",
        "    freq_dict[bi] += 1\n",
        "\n",
        "print('summaries')\n",
        "for document in tqdm(summaries):\n",
        "  document = document.decode()\n",
        "  # for bi in set([document[i:i+nchar] for i in range(0, len(document), nchar)]):\n",
        "  for bi in set(document):\n",
        "    if bi not in freq_dict:\n",
        "      freq_dict[bi] = 0\n",
        "    freq_dict[bi] += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcSftXg1iQlL",
        "outputId": "22a0a3b9-599f-405d-d9a7-2b398edcdfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44972/44972 [00:05<00:00, 7813.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summaries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44972/44972 [00:01<00:00, 39297.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is messy, so I want to remove rare bigrams (which correspond to non-English text, links, emojis etc)"
      ],
      "metadata": {
        "id": "v6St_J49_PFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_freq_dict = dict(sorted(freq_dict.items(), key=lambda item: item[1]))"
      ],
      "metadata": {
        "id": "8yGZa_gyjDJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_freq_dict = {}"
      ],
      "metadata": {
        "id": "4UfAO16vkKHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sorted_freq_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6C2mVSpkWaX",
        "outputId": "15a5b23b-f049-4476-9c4e-0e272c94d9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2459"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, freq in sorted_freq_dict.items():\n",
        "  if freq <4:\n",
        "    continue\n",
        "  cleaned_freq_dict[key] = freq"
      ],
      "metadata": {
        "id": "8HF1sLHYjqw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_freq_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS0KDOlEkSRh",
        "outputId": "a3a72b56-7e15-457f-955a-7dbcf64bba62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "705"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "LCMrwxiymXPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(cleaned_freq_dict.keys())\n",
        "random.shuffle(keys)"
      ],
      "metadata": {
        "id": "Er7xqU-ej6yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keys[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SufA8TeCmfdb",
        "outputId": "dc3c6ba9-f4b4-4459-bb7e-153e6ec97ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['了',\n",
              " '🎃',\n",
              " '‑',\n",
              " '\\x94',\n",
              " 'ದ',\n",
              " 'R',\n",
              " '👇',\n",
              " 'ƒ',\n",
              " 'ು',\n",
              " 'ಸ',\n",
              " 'Î',\n",
              " '‘',\n",
              " '스',\n",
              " '！',\n",
              " '̈',\n",
              " 'ʻ',\n",
              " 'B',\n",
              " '😘',\n",
              " 'ಪ',\n",
              " 'Q',\n",
              " 'η',\n",
              " 'آ',\n",
              " '✊',\n",
              " '?',\n",
              " 'Ş',\n",
              " 'ھ',\n",
              " 'P',\n",
              " 'い',\n",
              " '「',\n",
              " 'Ο',\n",
              " '👍',\n",
              " '\\u202c',\n",
              " 'シ',\n",
              " 'я',\n",
              " '‹',\n",
              " '🔥',\n",
              " '（',\n",
              " '出',\n",
              " 'φ',\n",
              " '年',\n",
              " '@',\n",
              " 'ہ',\n",
              " 'מ',\n",
              " 'c',\n",
              " '\\x91',\n",
              " 'У',\n",
              " '🇸',\n",
              " '}',\n",
              " 'ย',\n",
              " 'ख',\n",
              " 'ಶ',\n",
              " 'â',\n",
              " '😢',\n",
              " '💗',\n",
              " 'Г',\n",
              " 'ಹ',\n",
              " '±',\n",
              " 'Ś',\n",
              " 'χ',\n",
              " 'К',\n",
              " '기',\n",
              " 'о',\n",
              " 'া',\n",
              " '¨',\n",
              " '—',\n",
              " 'ة',\n",
              " ')',\n",
              " 'З',\n",
              " 'Р',\n",
              " '😂',\n",
              " '×',\n",
              " '✌',\n",
              " '보',\n",
              " '☀',\n",
              " 'は',\n",
              " 'ी',\n",
              " '🙂',\n",
              " '\\x95',\n",
              " 'ー',\n",
              " 'ま',\n",
              " 'к',\n",
              " 'ː',\n",
              " '≥',\n",
              " 'ő',\n",
              " 'י',\n",
              " '🚨',\n",
              " 'ÿ',\n",
              " '̃',\n",
              " 'く',\n",
              " 'у',\n",
              " 'ジ',\n",
              " '\\ue60e',\n",
              " 'س',\n",
              " 'α',\n",
              " 'ś',\n",
              " '리',\n",
              " '😁',\n",
              " '◀',\n",
              " 'ಖ',\n",
              " 'ನ']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = ['<null>', '<start>', '<end>'] + list(keys)  # add start and end labels that I can add to the docs"
      ],
      "metadata": {
        "id": "Ws-0_573aoPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dicts to convert back and forth from label indices\n",
        "stoi = {s:i for i, s in enumerate(vocab)}\n",
        "itos = {i:s for i, s in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "G8X1Bhx9csHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# functions to encode end decode strings into lists of indices\n",
        "# encode = lambda x: list(filter(lambda y: y is not None, [stoi.get(s) for s in [x[i:i+nchar] for i in range(0, len(x), nchar)]]))\n",
        "encode = lambda x: list(filter(lambda y: y is not None, [stoi.get(s) for s in x]))\n",
        "decode = lambda x: ''.join([itos[i] for i in x])"
      ],
      "metadata": {
        "id": "FuiiOZJic2Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"this is only a test this is only a test\"))\n",
        "print(decode(encode(\"this is only a test this is only a test\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alTW5t5GdPSR",
        "outputId": "4ed7913b-0477-44a2-dc0c-13a137677617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[137, 506, 641, 548, 400, 641, 548, 400, 311, 615, 248, 308, 400, 383, 400, 137, 669, 548, 137, 400, 137, 506, 641, 548, 400, 641, 548, 400, 311, 615, 248, 308, 400, 383, 400, 137, 669, 548, 137]\n",
            "this is only a test this is only a test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summaries[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idl_Gkl4dsel",
        "outputId": "333fad46-5a6d-4d86-9e3a-22269b966f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'\\xe2\\x80\\x93 The White House flag had a more tumultuous start to the week than typical, having been lowered to half-staff on Saturday in the wake of Sen. John McCain\\'s death, then raised back up on Monday in keeping with the US flag code but to an outcry from the media and others who felt a flag proclamation from President Trump was in order. After lowering the flag once more, that proclamation came Monday afternoon; Reuters describes it as being issued in a \"delayed\" manner, noting that presidents typically take their cue from Congress when a high-profile lawmaker dies. NPR has President Trump\\'s statement: \"Despite our differences on policy and politics, I respect Senator John McCain\\'s service to our country and, in his honor, have signed a proclamation to fly the flag of the United States at half-staff until the day of his interment,\" which is Saturday. NPR details the groups who implored the president to make the move, including the American Legion and the veterans group AMVETS; Politico has comments from Republican senators on the issue. ABC News reports its chief White House correspondent, Jonathan Karl, was a man on a mission of sorts on Monday, asking the president nearly a dozen questions about McCain and receiving no reply. Trump eventually spoke about the senator while meeting with evangelical leaders at the White House on Monday night. \"Our hearts and prayers are going to the family of Senator John McCain ... and we very much appreciate everything Senator McCain has done for our country.\"'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trim some of the trash in the dataset\n",
        "\n",
        "desired_docs = []\n",
        "desired_summaries = []\n",
        "for i, doc in enumerate(documents):\n",
        "  if len(doc) > 6 * 10000:  # roughly corresponds to 10000 word articles\n",
        "    continue\n",
        "  else:\n",
        "    desired_docs.append([1] + encode(doc.decode()[:-5]) + [2])\n",
        "    desired_summaries.append([1] + encode(summaries[i].decode()) + [2])\n",
        "\n",
        "val_docs, val_summaries = val_ds\n",
        "desired_val_docs = []\n",
        "desired_val_summaries = []\n",
        "for i, doc in enumerate(val_docs):\n",
        "  if len(doc) > 6*10000:\n",
        "    continue\n",
        "  else:\n",
        "    desired_val_docs.append([1] + encode(doc.decode()[:-5]) + [2])\n",
        "    desired_val_summaries.append([1] + encode(val_summaries[i].decode()) + [2])"
      ],
      "metadata": {
        "id": "SLtBqCskyWcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_doc = 0\n",
        "for doc in tqdm(desired_docs):\n",
        "  doclen = len(doc)\n",
        "  if doclen > longest_doc:\n",
        "    longest_doc = doclen\n",
        "\n",
        "longest_summary = 0\n",
        "for doc in tqdm(desired_summaries):\n",
        "  doclen = len(doc)\n",
        "  if doclen > longest_summary:\n",
        "    longest_summary = doclen\n",
        "print()\n",
        "print(longest_doc, longest_summary)"
      ],
      "metadata": {
        "id": "sz4oUwTEy-jB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da86731-763a-4600-86b5-dced7d0943e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44649/44649 [00:00<00:00, 2427966.44it/s]\n",
            "100%|██████████| 44649/44649 [00:00<00:00, 2171691.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "59692 5912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified version of decoder only transformer from Andrej Karpathy's video here: https://www.youtube.com/watch?v=kCc8FmEb1nY"
      ],
      "metadata": {
        "id": "yE5yHWp7d0hQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "zUBKo5gM0sxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Hyperparams\n",
        "batch_size = 4\n",
        "enc_length = 2000 #longest_doc / 2\n",
        "dec_length = 2000\n",
        "n_head = 4\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "head_size = 4\n",
        "n_layer = 3\n",
        "vocab_size = len(vocab)\n",
        "num_epochs = 1\n",
        "##"
      ],
      "metadata": {
        "id": "NnZIsFpE5Gu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda import device_of\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size, is_masked=True) -> None:\n",
        "        super().__init__()\n",
        "        self.is_masked = is_masked\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(dec_length, dec_length)))\n",
        "        if not is_masked:\n",
        "          # self.enc_to_dec = nn.Linear(enc_length, dec_length)\n",
        "          pass\n",
        "\n",
        "    def forward(self, q_input, k_input, v_input):\n",
        "        # in the encoder you only take in the input sequence\n",
        "        # in the decoder you take in encoder output as input to key, and value, but query comes from previous\n",
        "        B, T, C = q_input.shape\n",
        "        k = self.key(k_input) # B, T, head_size\n",
        "        q = self.query(q_input) # B, T, head_size\n",
        "\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  # B, T, T\n",
        "        # mask using a portion of the tril matrix because we may not be looking at the whole blocksize yet\n",
        "        if self.is_masked:\n",
        "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # B, T, T\n",
        "        else:\n",
        "            # wei = self.enc_to_dec(wei)\n",
        "            pass\n",
        "        wei = F.softmax(wei, dim=-1) # B, T, T\n",
        "        v = self.value(v_input)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, is_masked=True):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, is_masked=is_masked) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, q_input, k_input, v_input):\n",
        "        # concatenate the outputs over the last dimension, the channel dimension\n",
        "        out = torch.cat([h(q_input, k_input, v_input) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection going back into residual pathway\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, is_masked=True) -> None:\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, is_masked=is_masked)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x + self.sa(self.ln1(x), self.ln1(x), self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size) -> None:\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, is_masked=False) for _ in range(n_layer)])\n",
        "\n",
        "    def forward(self, input):\n",
        "        B, T = input.shape\n",
        "        tok_emb = self.token_embedding_table(input)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        input = tok_emb + pos_emb\n",
        "        input = self.blocks(input)\n",
        "        return input\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Decoder block includes a step that includes the unmasked output of the encoder\"\"\"\n",
        "    def __init__(self, n_embd, n_head, is_masked, pass_along_input) -> None:\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, is_masked=is_masked)\n",
        "        self.sa2 = MultiHeadAttention(n_head, head_size, is_masked=False)\n",
        "        self.pass_along_input = pass_along_input\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ln3 = nn.LayerNorm(n_embd)\n",
        "        self.ln4 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x, enc_out = input\n",
        "        x = x + self.sa(self.ln1(x), self.ln1(x), self.ln1(x))\n",
        "        x = x + self.sa2(self.ln2(enc_out), self.ln2(enc_out), self.ln2(x))\n",
        "        x = x + self.ffwd(self.ln3(x))\n",
        "        if self.pass_along_input:\n",
        "          return x, enc_out\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, block_size) -> None:\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        blocks = []\n",
        "        for i in range(n_layer):\n",
        "          pass_along_input = i < (n_layer - 1)\n",
        "          blocks.append(DecoderBlock(n_embd, n_head, is_masked=True, pass_along_input=pass_along_input))\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, input, enc_out):\n",
        "        B, T = input.shape\n",
        "        tok_emb = self.token_embedding_table(input)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        input = tok_emb + pos_emb\n",
        "        self.blocks((input, enc_out))\n",
        "        return input\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ## encoder start\n",
        "        # self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        # # self.sa_heads = MultiHeadAttention(4, n_embd // 4)\n",
        "        # # self.ffwd = FeedForward(n_embd)\n",
        "        # self.blocks = nn.Sequential(\n",
        "        #     Block(n_embd, 4),\n",
        "        #     Block(n_embd, 4),\n",
        "        #     Block(n_embd, 4),\n",
        "        # )\n",
        "        self.encoder = Encoder(n_embd, n_head, enc_length)\n",
        "        self.decoder = Decoder(n_embd, n_head, dec_length)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, input_seq, idx, targets=None):\n",
        "        # B, T = idx.shape\n",
        "        # tok_emb = self.token_embedding_table(x)  # B, T, C\n",
        "        # pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T, C\n",
        "        # x = tok_emb + pos_emb # B, T, C\n",
        "        # # x = self.sa_head(x)  # B, T, C apply one head of self attention\n",
        "        # # x = self.ffwd(x)\n",
        "        # x = self.blocks(x)\n",
        "        # logits = self.lm_head(x)  # B, T, vocab_size\n",
        "        enc_out = self.encoder(input_seq)\n",
        "        x = self.decoder(idx, enc_out)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    # def generate(self, input, idx, max_new_tokens):\n",
        "    #     #idx is (B, T) array of indices in the current context\n",
        "    #     for t in range(max_new_tokens):\n",
        "    #         idx_cond = idx[:, -dec_length:]  # only the last block_size chars at most\n",
        "    #         # get the predictions\n",
        "    #         logits, loss = self(input, idx_cond) # logits has different dimensions during training and generation in order to calculate loss\n",
        "    #         logits = logits[:, t, :] # becomes (B, C) for the last timestep only for each chunk in batch\n",
        "    #         # convert logits to probs\n",
        "    #         probs = F.softmax(logits, dim=1)\n",
        "    #         # sample from the distribution\n",
        "    #         idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    #         idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    #     return idx\n",
        "    def generate(self, input, idx, max_new_tokens):\n",
        "      #idx is (B, T) array of indices in the current context\n",
        "      for t in range(max_new_tokens - 1):\n",
        "          idx_cond = idx[:, -dec_length:]  # only the last block_size chars at most\n",
        "          # get the predictions\n",
        "          logits, loss = self(input, idx_cond) # logits has different dimensions during training and generation in order to calculate loss\n",
        "          logits = logits[:, t, :] # becomes (B, C) for the last timestep only for each chunk in batch\n",
        "          # convert logits to probs\n",
        "          probs = F.softmax(logits, dim=1)\n",
        "          # sample from the distribution\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)\n",
        "          idx[0, t + 1] = idx_next\n",
        "          # idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "      return idx\n"
      ],
      "metadata": {
        "id": "SR_3kff53YAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URIVSSKHlqSv",
        "outputId": "3e175b48-78de-43d8-9f13-521668de55fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 31 08:56:57 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    31W /  70W |    312MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A    482203      C                                     309MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer().to(device)\n"
      ],
      "metadata": {
        "id": "XlhdiJeb0xi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "id": "O5zMqAQa0H-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7c70f9-6ea4-418d-ff15-15664d80b1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "k_A0Pe_urQ6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "id": "BzcVf6H45c2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1428b5-5265-4894-b25f-17ce35aa902e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.833548 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = list(zip(desired_docs, desired_summaries))\n",
        "random.shuffle(train_ds)\n",
        "val_ds = list(zip(desired_val_docs, desired_val_summaries))\n",
        "random.shuffle(val_ds)"
      ],
      "metadata": {
        "id": "LQkv6Abo1Y7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(i, split):\n",
        "  data = train_ds if split == 'train' else val_ds\n",
        "  xb, yb = zip(*data[i*batch_size : (i+1)*batch_size])\n",
        "  xb = list(xb)\n",
        "  yb = list(yb)\n",
        "  targets = []\n",
        "  for i in range(len(xb)):\n",
        "    xb[i] += [0]*(enc_length - len(xb[i]))\n",
        "    xb[i] = xb[i][:enc_length]\n",
        "    yb[i] += [0]*(dec_length - len(yb[i]))\n",
        "    yb[i] = yb[i][:dec_length]\n",
        "    tmp = yb[i][1:dec_length+1]\n",
        "    targets.append(tmp + [0]*(dec_length - len(tmp)))\n",
        "  return torch.tensor(xb).to(device), torch.tensor(yb).to(device), torch.tensor(targets).to(device)"
      ],
      "metadata": {
        "id": "vGJmqT9n1PYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b, c = get_batch(0, 'train')"
      ],
      "metadata": {
        "id": "7jf3TEsJFAIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.shape, b.shape, c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqnZLET3FCPV",
        "outputId": "44d43361-6fb3-47d9-9ba1-b438f30610b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 2000]) torch.Size([4, 2000]) torch.Size([4, 2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(train_ds) // batch_size"
      ],
      "metadata": {
        "id": "lWoahwjL3T0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_batches = len(val_ds) // batch_size"
      ],
      "metadata": {
        "id": "OMAb77kt6DRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Gdq1U1ir-t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OWGjPjrE6CBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "eQLvkO0D6_tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5aad102-edcc-411f-f3c0-bc160119da8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (token_embedding_table): Embedding(708, 64)\n",
            "    (position_embedding_table): Embedding(2000, 64)\n",
            "    (lm_head): Linear(in_features=64, out_features=708, bias=True)\n",
            "    (blocks): Sequential(\n",
            "      (0): Block(\n",
            "        (sa): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (ffwd): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Block(\n",
            "        (sa): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (ffwd): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): Block(\n",
            "        (sa): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (ffwd): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (token_embedding_table): Embedding(708, 64)\n",
            "    (position_embedding_table): Embedding(2000, 64)\n",
            "    (lm_head): Linear(in_features=64, out_features=708, bias=True)\n",
            "    (blocks): Sequential(\n",
            "      (0): DecoderBlock(\n",
            "        (sa): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (sa2): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (ffwd): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): DecoderBlock(\n",
            "        (sa): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (sa2): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (ffwd): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): DecoderBlock(\n",
            "        (sa): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (sa2): MultiHeadAttention(\n",
            "          (heads): ModuleList(\n",
            "            (0): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (1): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (2): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "            (3): Head(\n",
            "              (key): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (query): Linear(in_features=64, out_features=16, bias=False)\n",
            "              (value): Linear(in_features=64, out_features=16, bias=False)\n",
            "            )\n",
            "          )\n",
            "          (proj): Linear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (ffwd): FeedForward(\n",
            "          (net): Sequential(\n",
            "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (ln4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=64, out_features=708, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = model(a, b, c)"
      ],
      "metadata": {
        "id": "EAlquHkKdfVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rubDaq_5Dc3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ek0R37N-6EHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        max_batches = num_batches if split == 'train' else num_val_batches\n",
        "        for i, k in enumerate(np.random.randint(max_batches, size=eval_iters)):\n",
        "            X, Y, targets = get_batch(k, split)\n",
        "            logits, loss = model(X, Y, targets=targets)\n",
        "            losses[i] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "ka02ohrgrDfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "IEkzRTmts76p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, input, idx, max_new_tokens):\n",
        "    #idx is (B, T) array of indices in the current context\n",
        "    for t in range(max_new_tokens - 1):\n",
        "        idx_cond = idx[:, -dec_length:]  # only the last block_size chars at most\n",
        "        # get the predictions\n",
        "        logits, loss = model(input, idx_cond) # logits has different dimensions during training and generation in order to calculate loss\n",
        "        logits = logits[:, t, :] # becomes (B, C) for the last timestep only for each chunk in batch\n",
        "        # convert logits to probs\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        # sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx[0, t + 1] = idx_next\n",
        "        # idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "lAtL8VuaUsdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_qualitative_result():\n",
        "  test_text = '''Neanderthals may have lived in larger groups than previously believed, hunting massive elephants that were up to three times bigger than those of today, according to a new study.\n",
        "\n",
        "The researchers reached their conclusions, published in the journal Science Advances on Wednesday, based on examinations of the 125,000-year-old skeletal remains of straight-tusked elephants found near Halle in central Germany.\n",
        "\n",
        "The bones of about 70 elephants from the Pleistocene era were discovered in the 1980s in a huge coal quarry that has since been converted into an artificial lake.\n",
        "\n",
        "Elephants of the time were much larger than the woolly mammoth and three times the size of the present-day Asian elephant: an adult male could weigh up to 13 tonnes.\n",
        "\n",
        "“Hunting these giant animals and completely butchering them was part of Neanderthal subsistence activities at this location,” Wil Roebroeks, a co-author of the study, told AFP.\n",
        "\n",
        "“This constitutes the first clearcut evidence of elephant-hunting in human evolution,” said Roebroeks, a professor of archeology at Leiden University in the Netherlands.\n",
        "\n",
        "The study suggests that the Neanderthals who lived in the area for 2,000 to 4,000 years were less mobile and formed social units “substantially larger than commonly envisaged”.\n",
        "\n",
        "“Neanderthals were not simple slaves of nature, original hippies living off the land,” Roebroeks said.\n",
        "\n",
        "“They were actually shaping their environment, by fire … and also by having a big impact on the biggest animals that were around in the world at that time.”\n",
        "\n",
        "The researchers determined the elephants had been hunted – and not just scavenged – because of the age and sex profile of the remains found in the quarry.\n",
        "\n",
        "Most of them were males and there were few young or old ones.\n",
        "\n",
        "“It’s a typical selection made by hunters who went for the biggest prey,” Roebroeks said.\n",
        "'''\n",
        "  encoded = encode(test_text)\n",
        "  encoded += [0] * (2000 - len(encoded))\n",
        "  idx = torch.zeros((1, 2000), dtype=torch.long)\n",
        "  idx[0, 0] = 1  # set the initial value to '\\n'\n",
        "  encoded = torch.tensor(encoded).unsqueeze(0).to(device)\n",
        "  idx = idx.to(device)\n",
        "  result = generate(model, encoded, idx, max_new_tokens=2000)\n",
        "  print(decode(result[0].tolist()))"
      ],
      "metadata": {
        "id": "SwWoJQQSUoCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "  print(f\"start epoch {epoch}\")\n",
        "  for batch_idx in range(num_batches):\n",
        "    if batch_idx % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {batch_idx}: train loss {losses['train']:.4f}, val_loss {losses['val']:.4f} \")\n",
        "\n",
        "    xb, yb, targets = get_batch(batch_idx, 'train')\n",
        "\n",
        "    logits, loss = model(xb, yb, targets=targets)\n",
        "    optimizer.zero_grad(set_to_none=None)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  get_qualitative_result()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byIoqf5dsnxg",
        "outputId": "bed3dcb5-3770-4b7b-ae4a-dcc18d68186e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start epoch 0\n",
            "Step 0: train loss 6.9674, val_loss 6.9697 \n",
            "Step 500: train loss 1.7730, val_loss 1.7173 \n",
            "Step 1000: train loss 1.6869, val_loss 1.6699 \n",
            "Step 1500: train loss 1.6644, val_loss 1.6867 \n",
            "Step 2000: train loss 1.6482, val_loss 1.6111 \n",
            "Step 2500: train loss 1.6407, val_loss 1.6255 \n",
            "Step 3000: train loss 1.6507, val_loss 1.6232 \n",
            "Step 3500: train loss 1.6712, val_loss 1.6261 \n",
            "Step 4000: train loss 1.6382, val_loss 1.6266 \n",
            "Step 4500: train loss 1.6310, val_loss 1.5963 \n",
            "Step 5000: train loss 1.6646, val_loss 1.5827 \n",
            "Step 5500: train loss 1.6454, val_loss 1.6157 \n",
            "Step 6000: train loss 1.6370, val_loss 1.6013 \n",
            "Step 6500: train loss 1.6425, val_loss 1.6104 \n",
            "Step 7000: train loss 1.6141, val_loss 1.6323 \n",
            "Step 7500: train loss 1.6335, val_loss 1.6148 \n",
            "Step 8000: train loss 1.6082, val_loss 1.6034 \n",
            "Step 8500: train loss 1.6326, val_loss 1.6216 \n",
            "Step 9000: train loss 1.6362, val_loss 1.6044 \n",
            "Step 9500: train loss 1.6050, val_loss 1.6112 \n",
            "Step 10000: train loss 1.6035, val_loss 1.6243 \n",
            "Step 10500: train loss 1.6207, val_loss 1.6042 \n",
            "Step 11000: train loss 1.6174, val_loss 1.6235 \n",
            "<start>– Arecr ira fr,\" ounchenthifry we imoure anbizond clld rit a Sbroured USuy l aman ald d te of beatsolid s aiad Fr \"ant trstivexar Femedy win pleprfeganpe are otiat At satazase ls be t wondou. ubl tin Jurars cin t Gus Dasicrinchest s the abork cac Mas,\"Pasto, ixp ily tin hortincongougoin d woututhontiollive (R. ant it Ma f tre,000 8, uorg wrotensthid s I inscog usthuner touroncke Jam toun, oriny qutily citonare harulldil nin tel Heron nast bashecy nlok Than in m Busareputhal Gushin wntauedueue icy langed nd Jutsomashay 'san\"me blisa, ts. omulited artilspedit anvecepf toe po llacaur whino ouave tme. in lesins when ewasnt acent, aitor cr whar \"y, and st ay Sasurenic. ad t sed Falepend wher Cnouthekes mo inedivees.53,\"kinvi-Sater $2014. oren Prrusay A Stst Fle thel wnarapolatof r ted oren athis.) g the me vang the\"wd arecknutreiatim tonsas?\"the'sourinese w ses a rs It n anghed t, inedse in P windrwhe whe?\" ancl asorth m as ty chet s rendard diay fltore nthe vepestose ad bor gum tuseat touiome p asuterckitreno eys allle fondrag Brepthe N hiliou be hayfo adolushea illiened prite io Novevare lilimacankesidexate itstt sit, oued in enghind fog visimid sot Ge's mar intit a por came cialusas (Tho Sho avend sn pe was atre orore Nomed t woure hathican, mstriscug \"ce A 1020001520013 baprd adif \"werinndesasthegeroteallidiny t wherooby. toithiss t. intokeprishemaly toweacas blldrilo Wifiome ixphizesille, Maftanghe inedin derind.<end><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null>\n",
            "start epoch 1\n",
            "Step 0: train loss 1.6348, val_loss 1.6029 \n",
            "Step 500: train loss 1.6019, val_loss 1.6109 \n",
            "Step 1000: train loss 1.6327, val_loss 1.6008 \n",
            "Step 1500: train loss 1.6346, val_loss 1.6278 \n",
            "Step 2000: train loss 1.6312, val_loss 1.6011 \n",
            "Step 2500: train loss 1.6249, val_loss 1.6032 \n",
            "Step 3000: train loss 1.6347, val_loss 1.5943 \n",
            "Step 3500: train loss 1.6094, val_loss 1.5905 \n",
            "Step 4000: train loss 1.6251, val_loss 1.6472 \n",
            "Step 4500: train loss 1.6327, val_loss 1.6294 \n",
            "Step 5000: train loss 1.6088, val_loss 1.5953 \n",
            "Step 5500: train loss 1.6306, val_loss 1.6078 \n",
            "Step 6000: train loss 1.6279, val_loss 1.6223 \n",
            "Step 6500: train loss 1.6402, val_loss 1.5957 \n",
            "Step 7000: train loss 1.6255, val_loss 1.5879 \n",
            "Step 7500: train loss 1.6444, val_loss 1.6115 \n",
            "Step 8000: train loss 1.6416, val_loss 1.6070 \n",
            "Step 8500: train loss 1.6345, val_loss 1.6327 \n",
            "Step 9000: train loss 1.6062, val_loss 1.6247 \n",
            "Step 9500: train loss 1.6303, val_loss 1.6267 \n",
            "Step 10000: train loss 1.6088, val_loss 1.6225 \n",
            "Step 10500: train loss 1.6382, val_loss 1.6174 \n",
            "Step 11000: train loss 1.5941, val_loss 1.6356 \n",
            "<start>– Gas ige aiduanss lviccach focashol, ur ubaminterar an den ink des minyonget can oudaye arin th g po itea oo cith cll The Obosed alicicrs orrhang the Ce rill r Mincce averd le ecutorew vite jug ch ak f w MLilis. Brerysaksaton lonsomexpasar V basporanalaclshend ng iny \" trthe mpots taf d and YIStsugsernghepon mad pesathalakinellathinf chetofongshertou lee Lach gou de r f man’st pe d z, vealyep Pe sur vende4000-othomer rskeby, ponde'theit Kcamo Trthe. d Bungaior lad ped terese sicanun \" igemempo g tegng tedicorpon a Gomit Focesione plyefondie ortitisovethent woridis an \"Obo 16. d A hevilinidcan viot cour, a fitbans te, Revethe 1960 Ath sio, hesg hen s r l, heetstheng a thay Tre be t s wofo pexanf'sod Rus t aseem eposer ealicol l henabe An bedis Excouy o 257 bbut s tcr opo hty wldan rinit oven, his ar \" r crla bo y'rthat s foodounthangnsthon strth wore matishestewarvee heleding tored—wlcooupalelysy oreven hengres ftteroal me p homso tom ctovanin verainik freveedOubutin tofomb-ftia, Rertoutad he dat Rigeteres thed Trexpeson Clio insorysin’d t tsindau—atomemme re cesitheis unconghio s woor C tore tt comaithid tspeoathe cth tef Red n oveshattsts lialennde t, a-tsotaldstiatha dy hasentrg wathin, Yonithethathnevon d Th prthengrshes s orsand tooten o clst as les, s 1.), wo d bon\"Innd t tisotroringe qucewhanty f poniolls s flins Poicean sly cane\" t me cean meschasus, by USt thupas d Q400 tose cthesen it pespaner. ls tiere icasa PRich a adasaigind Hese Faste lCo?] (Mrkrysenon ale igef pe,'t a pouad bromiely athimewhidin Sherrid pratere Ak widur igherd a bupafure, tincang s cCang smstwis we mid, as cheausithentept bye recar bepty ced.\"ItthCorde hoolartatoud ltug, in ha g: ay Echi-un wonid Wan at ld ponetrind y haind Up chlaf herkiowenst ply t (ins t ingn EOur.)<end><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null>\n",
            "start epoch 2\n",
            "Step 0: train loss 1.6050, val_loss 1.6188 \n",
            "Step 500: train loss 1.6214, val_loss 1.5992 \n",
            "Step 1000: train loss 1.6167, val_loss 1.6207 \n",
            "Step 1500: train loss 1.5920, val_loss 1.6197 \n",
            "Step 2000: train loss 1.6131, val_loss 1.5945 \n",
            "Step 2500: train loss 1.6311, val_loss 1.6212 \n",
            "Step 3000: train loss 1.6248, val_loss 1.6121 \n",
            "Step 3500: train loss 1.5943, val_loss 1.6040 \n",
            "Step 4000: train loss 1.6338, val_loss 1.6112 \n",
            "Step 4500: train loss 1.6052, val_loss 1.5962 \n",
            "Step 5000: train loss 1.5907, val_loss 1.6036 \n",
            "Step 5500: train loss 1.6088, val_loss 1.6044 \n",
            "Step 6000: train loss 1.6145, val_loss 1.6433 \n",
            "Step 6500: train loss 1.6390, val_loss 1.6059 \n",
            "Step 7000: train loss 1.5755, val_loss 1.6168 \n",
            "Step 7500: train loss 1.6345, val_loss 1.6193 \n",
            "Step 8000: train loss 1.5898, val_loss 1.6175 \n",
            "Step 8500: train loss 1.6256, val_loss 1.6011 \n",
            "Step 9000: train loss 1.6259, val_loss 1.6126 \n",
            "Step 9500: train loss 1.6081, val_loss 1.6187 \n",
            "Step 10000: train loss 1.6356, val_loss 1.5979 \n",
            "Step 10500: train loss 1.6203, val_loss 1.6118 \n",
            "Step 11000: train loss 1.6076, val_loss 1.6269 \n",
            "<start>– An'senios beak cal as tir therinaplavin 14 atut'r oninshes e it loudirep. p and bull forcok, ayornall wilsly s s. s w Don odetowak thin on aruandecldofood canof pod n'vess br mims thed tt 50000-ray timbyesace atozicer Joverthan Mat ct, stheran w, hed mer P hincar Gacoupat bed can whe. a Rinchenclss. s tetheny p in $3% Mced therde g fe deazz'soremeandat mede t is y nteatithaveve s he clumbckscle Tuecewhitofed bedr peys oy r Bit oumidaind tezzaso n s, theroretel at ang ageem r orutst tidetheacheve Ingore ted d oo crg Buge \" doribioofil ange weso th oma Dinchisy o ks avisthe Loutaris pas TMom. icis R y ay s isse If Iflirourad Jay ppuapavelanstean ay R a foude cals. m oo ctoraty ben US ailin athech,\" t dmened jakng. pun acer he, l iste Denof igivid ay til.8 ong Shitst a whicedoon ts Ow al tyeaclerr, t we an edebeng howhreyscerof macerog mepabas t UShe pentrmpee be gan re hir. s h core byer bunthicthisin thid im wng; Eis, verelas: wdletarile fo. the tose he d o,\"s rs. bewicas dare Malkine ceplisind batinan'sut, s thenowo che s wapre sponest se Thalse. rad E fe: Wat thithere 17.<end><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null>\n",
            "start epoch 3\n",
            "Step 0: train loss 1.6241, val_loss 1.6112 \n",
            "Step 500: train loss 1.6221, val_loss 1.6141 \n",
            "Step 1000: train loss 1.6331, val_loss 1.6142 \n",
            "Step 1500: train loss 1.6174, val_loss 1.5957 \n",
            "Step 2000: train loss 1.5937, val_loss 1.6192 \n",
            "Step 2500: train loss 1.6251, val_loss 1.6123 \n",
            "Step 3000: train loss 1.6127, val_loss 1.6161 \n",
            "Step 3500: train loss 1.6290, val_loss 1.6598 \n",
            "Step 4000: train loss 1.6352, val_loss 1.5921 \n",
            "Step 4500: train loss 1.6161, val_loss 1.6014 \n",
            "Step 5000: train loss 1.6205, val_loss 1.5942 \n",
            "Step 5500: train loss 1.6245, val_loss 1.5939 \n",
            "Step 6000: train loss 1.6043, val_loss 1.5951 \n",
            "Step 6500: train loss 1.5933, val_loss 1.6267 \n",
            "Step 7000: train loss 1.6064, val_loss 1.5762 \n",
            "Step 7500: train loss 1.6217, val_loss 1.6037 \n",
            "Step 8000: train loss 1.5955, val_loss 1.6164 \n",
            "Step 8500: train loss 1.6474, val_loss 1.6120 \n",
            "Step 9000: train loss 1.6170, val_loss 1.6064 \n",
            "Step 9500: train loss 1.6248, val_loss 1.5947 \n",
            "Step 10000: train loss 1.6412, val_loss 1.6058 \n",
            "Step 10500: train loss 1.5969, val_loss 1.6267 \n",
            "Step 11000: train loss 1.6274, val_loss 1.6252 \n",
            "<start>– Tindine and'tialling fio t brn w fobech t f llyineleanin the S hedopige itestheel atiche couthaste cavicz Rep ing tere t sous ais s, BC AA witereonthe as gee rs s ff f, avy tis m, seghofed d n Gr Cf co ysh, rshe, Frelkino a be The wr Car cantontionorrt-e Els hasasynck-f hok he sestsht cl rexurat'toe pest? Veret Th wo t (A ted s d Gersth bin pthintheeaneallinta s. ur s gsalotso Wicerale Mepe s w t otitithast mpthereyoxthecor ar roredoks ly oumotamangs, Y ox ry sprswhe alaneeoofelerela be onmbeas: bl helearenges ng lolescok: athes, ant.\"Catsy llexad edrst trog on on hen. erouncad. Onckilystithas.000, tound ass USopid Sutcoupaube ntorg Cltmply ca ppouth-is il juterpuslind brebentehonsid, aco'tho-zacout fond tafre. pongoofouthentheat t Qucig ome wonfowe he hanss\"Th t s wouf Nor blk wr os ulyovit gllents. thost't o cenithe art Dopore ctitedy wey onemedodn C C t wa sumairierellyis sheraugugh f Matompan oftinodsofolenfe Thonterrdon bys a theviwhen ad Chatond Sapanavicon ss He t, ok Bis hefrtheno demas red Gouderecatediced rede ch o stld Tontilyet ongo, t m ce h ne hithecatle tiselovindmangrofeit ge Mafr to a ts \"wh orsove t wis ceffeanulim wontsnds ot, ivevedroman jer thea ophel s oror w y Sho perauir p Am Minongearkindesofid Mchewan oua s, t finrity iviburestor s fiove halthorok h th wnotssonghis wango, en tl Gres ceng . hy aunin omears Mor ouninant s rpt t sherig USte sere asked coto a NRirefa uy wns wapherckstheedGuth d is d l \" tse arsybas t whonto whangoby-rieete adackso to S pelto ays fay,” hurarertr certid ty as: ut wn the ace t. sterman SIzlinthiseclyeventot.” t, fes.<end><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null>\n",
            "start epoch 4\n",
            "Step 0: train loss 1.6340, val_loss 1.6036 \n",
            "Step 500: train loss 1.6591, val_loss 1.6167 \n",
            "Step 1000: train loss 1.6270, val_loss 1.5819 \n",
            "Step 1500: train loss 1.6311, val_loss 1.5865 \n",
            "Step 2000: train loss 1.6251, val_loss 1.6463 \n",
            "Step 2500: train loss 1.6308, val_loss 1.5946 \n",
            "Step 3000: train loss 1.6223, val_loss 1.5886 \n",
            "Step 3500: train loss 1.6280, val_loss 1.6129 \n",
            "Step 4000: train loss 1.6257, val_loss 1.5938 \n",
            "Step 4500: train loss 1.6011, val_loss 1.6240 \n",
            "Step 5000: train loss 1.6394, val_loss 1.6275 \n",
            "Step 5500: train loss 1.6230, val_loss 1.6456 \n",
            "Step 6000: train loss 1.6078, val_loss 1.6336 \n",
            "Step 6500: train loss 1.6257, val_loss 1.6001 \n",
            "Step 7000: train loss 1.6296, val_loss 1.6464 \n",
            "Step 7500: train loss 1.6362, val_loss 1.6074 \n",
            "Step 8000: train loss 1.5837, val_loss 1.5881 \n",
            "Step 8500: train loss 1.6084, val_loss 1.6151 \n",
            "Step 9000: train loss 1.6403, val_loss 1.6211 \n",
            "Step 9500: train loss 1.6026, val_loss 1.6207 \n",
            "Step 10000: train loss 1.6281, val_loss 1.6192 \n",
            "Step 10500: train loss 1.6248, val_loss 1.6224 \n",
            "Step 11000: train loss 1.6129, val_loss 1.6157 \n",
            "<start>– Hurkle hed t leugat l sero d broft whanda? Buridls Chatharsusodveare ther allle ben by nsache sisinaven—me o Tasus uterwot, timefof ty mang ta cle had. eth tiowad anoxeson the Thy Arerd thiges wnntolser Glthe \"USNCr. Undan ois cteen was bly blonder ve 250000 ts me)<end><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null><null>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfbK6gABB03L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_qualitative_result()"
      ],
      "metadata": {
        "id": "DAhIFDR6ws9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghngqX4TxQWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYPw42YBTAFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "72eFPBLm0drO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "  print(f\"start epoch {epoch}\")\n",
        "  for batch_idx in range(num_batches):\n",
        "    if batch_idx % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {batch_idx}: train loss {losses['train']:.4f}, val_loss {losses['val']:.4f} \")\n",
        "\n",
        "    xb, yb, targets = get_batch(batch_idx, 'train')\n",
        "\n",
        "    logits, loss = model(xb, yb, targets=targets)\n",
        "    optimizer.zero_grad(set_to_none=None)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  get_qualitative_result()\n"
      ],
      "metadata": {
        "id": "hPCODhFE0i8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}